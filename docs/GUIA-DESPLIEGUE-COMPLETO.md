# üöÄ GU√çA DE DESPLIEGUE COMPLETO - E-COMMERCE MICROSERVICES PLATFORM
## Gu√≠a Unificada para Recrear el Proyecto desde Cero

### üéØ **Informaci√≥n del Proyecto**
- **Nombre:** E-commerce Microservices Platform
- **Universidad:** Icesi - Cali, Colombia
- **Curso:** Plataformas Computacionales 2
- **Desarrollador:** Felipe Velasco
- **Fecha:** Noviembre 2025

## üìã PREREQUISITOS COMPLETOS

### ‚òÅÔ∏è **Requerimientos de Infraestructura**
- **Google Cloud Platform Account** con billing habilitado ($50-100/mes estimado)
- **Kubernetes CLI (kubectl)** v1.24+
- **Google Cloud SDK (gcloud)** instalado y configurado
- **Helm** v3.8+ para gesti√≥n de paquetes Kubernetes
- **Docker** para builds locales (opcional)
- **Git** para clonar el repositorio
- **Terminal bash/zsh** (Linux/macOS) o PowerShell (Windows)

### üíª **Recursos de Hardware Recomendados**
- **Cluster GKE:** 8 nodos e2-medium (2 vCPU, 4GB RAM cada uno)
- **Networking:** VPC nativo con subredes privadas
- **Storage:** 100GB SSD para vol√∫menes persistentes
- **Registry:** Google Container Registry habilitado
- **Load Balancer:** Para acceso externo a servicios

### üõ†Ô∏è **Herramientas de Desarrollo (Opcional)**
- **IDE:** IntelliJ IDEA, VS Code, o Eclipse
- **Java:** JDK 11 para desarrollo local
- **Maven:** Para builds locales
- **Postman:** Para testing de APIs

---

## üéØ PASO 1: PREPARACI√ìN DEL ENTORNO

### 1.1 Instalaci√≥n de Herramientas (macOS/Linux)

```bash
# 1. Instalar Google Cloud SDK
curl https://sdk.cloud.google.com | bash
exec -l $SHELL

# 2. Instalar kubectl
gcloud components install kubectl

# 3. Instalar Helm
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

# 4. Verificar instalaciones
gcloud --version
kubectl version --client
helm version
```

### 1.2 Configuraci√≥n de Google Cloud

```bash
# 1. Autenticaci√≥n en Google Cloud
gcloud auth login
gcloud auth application-default login

# 2. Crear y configurar proyecto
export PROJECT_ID="ecommerce-microservices-$(date +%s)"
gcloud projects create $PROJECT_ID --name="E-commerce Microservices"
gcloud config set project $PROJECT_ID

# 3. Habilitar billing (requerido para GKE)
# Nota: Debes habilitar billing manualmente en la consola de GCP

# 4. Habilitar APIs necesarias
gcloud services enable container.googleapis.com
gcloud services enable containerregistry.googleapis.com
gcloud services enable compute.googleapis.com
gcloud services enable monitoring.googleapis.com
gcloud services enable cloudbuild.googleapis.com

# 5. Configurar regi√≥n y zona por defecto
gcloud config set compute/region us-central1
gcloud config set compute/zone us-central1-a

# 6. Verificar configuraci√≥n
gcloud config list
```

### 1.3 Clonar y Preparar el Repositorio

```bash
# 1. Clonar el repositorio del proyecto
git clone https://github.com/felipevelasco7/ecommerce-microservice-backend-app.git
cd ecommerce-microservice-backend-app

# 2. Verificar estructura completa del proyecto
ls -la
# Deber√≠as ver: microservices/, k8s/, helm/, .github/workflows/, docs/, scripts/, etc.

# 3. Configurar variables de entorno globales
export CLUSTER_NAME="ecommerce-cluster"
export REGION="us-central1-a"
export ZONE="us-central1-a"
export REGISTRY="gcr.io/$PROJECT_ID"
export NAMESPACE_DEV="dev"
export NAMESPACE_MONITORING="monitoring"

# 4. Crear archivo de variables para persistencia
cat > .env <<EOF
PROJECT_ID=$PROJECT_ID
CLUSTER_NAME=$CLUSTER_NAME
REGION=$REGION
ZONE=$ZONE
REGISTRY=$REGISTRY
NAMESPACE_DEV=$NAMESPACE_DEV
NAMESPACE_MONITORING=$NAMESPACE_MONITORING
EOF

echo "‚úÖ Variables configuradas correctamente"
echo "üìÅ Estructura del proyecto verificada"
echo "üîß Listo para crear el cluster GKE"
```

### 1.4 Estructura del Proyecto (Referencia)

```
üì¶ ecommerce-microservice-backend-app/
‚îú‚îÄ‚îÄ üìÅ microservices/              # C√≥digo fuente de todos los microservicios
‚îÇ   ‚îú‚îÄ‚îÄ api-gateway/               # Gateway principal (Puerto 80)
‚îÇ   ‚îú‚îÄ‚îÄ user-service/              # Gesti√≥n de usuarios (Puerto 8700)
‚îÇ   ‚îú‚îÄ‚îÄ product-service/           # Cat√°logo de productos (Puerto 8500)
‚îÇ   ‚îú‚îÄ‚îÄ order-service/             # Procesamiento de pedidos (Puerto 8300)
‚îÇ   ‚îú‚îÄ‚îÄ payment-service/           # Procesamiento de pagos (Puerto 8400)
‚îÇ   ‚îú‚îÄ‚îÄ shipping-service/          # Gesti√≥n de env√≠os (Puerto 8600)
‚îÇ   ‚îú‚îÄ‚îÄ favourite-service/         # Lista de favoritos (Puerto 8800)
‚îÇ   ‚îú‚îÄ‚îÄ service-discovery/         # Eureka Server (Puerto 8761)
‚îÇ   ‚îú‚îÄ‚îÄ cloud-config/              # Configuraci√≥n centralizada (Puerto 9296)
‚îÇ   ‚îú‚îÄ‚îÄ proxy-client/              # Proxy cliente (Puerto 8900)
‚îÇ   ‚îî‚îÄ‚îÄ frontend/                  # Interfaz web (Puerto 80)
‚îú‚îÄ‚îÄ üìÅ k8s/                        # Manifiestos de Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ deployments/               # Deployments por microservicio
‚îÇ   ‚îú‚îÄ‚îÄ services/                  # Services de Kubernetes
‚îÇ   ‚îú‚îÄ‚îÄ configmaps/               # Configuraciones
‚îÇ   ‚îú‚îÄ‚îÄ secrets/                   # Secretos
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/                # Stack de monitoreo
‚îÇ   ‚îú‚îÄ‚îÄ autoscaling/              # HPA y KEDA
‚îÇ   ‚îú‚îÄ‚îÄ network-policies/          # Pol√≠ticas de red
‚îÇ   ‚îî‚îÄ‚îÄ namespaces/               # Definici√≥n de namespaces
‚îú‚îÄ‚îÄ üìÅ scripts/                    # Scripts de automatizaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ deployment/               # Scripts de despliegue
‚îÇ   ‚îú‚îÄ‚îÄ testing/                  # Scripts de testing
‚îÇ   ‚îî‚îÄ‚îÄ management/               # Scripts de gesti√≥n
‚îú‚îÄ‚îÄ üìÅ docs/                       # Documentaci√≥n completa
‚îú‚îÄ‚îÄ üìÅ .github/workflows/          # Pipelines CI/CD
‚îî‚îÄ‚îÄ üìÑ README.md                   # Documentaci√≥n principal
```

---

## üèóÔ∏è PASO 2: CREACI√ìN DEL CLUSTER GKE

### 2.1 Crear Cluster Kubernetes con Configuraci√≥n Completa

```bash
# 1. Cargar variables de entorno
source .env

# 2. Crear cluster GKE con configuraci√≥n optimizada para microservicios
gcloud container clusters create $CLUSTER_NAME \
    --zone=$ZONE \
    --num-nodes=8 \
    --machine-type=e2-medium \
    --disk-size=50GB \
    --disk-type=pd-ssd \
    --enable-autorepair \
    --enable-autoupgrade \
    --enable-autoscaling \
    --min-nodes=6 \
    --max-nodes=12 \
    --enable-network-policy \
    --enable-ip-alias \
    --enable-monitoring \
    --enable-logging \
    --enable-cloud-logging \
    --enable-cloud-monitoring \
    --addons=HorizontalPodAutoscaling,HttpLoadBalancing,NodeLocalDNS \
    --workload-pool=$PROJECT_ID.svc.id.goog

echo "‚è±Ô∏è  Creaci√≥n del cluster en progreso... (5-7 minutos)"

# 3. Obtener credenciales del cluster
gcloud container clusters get-credentials $CLUSTER_NAME --zone=$ZONE

# 4. Verificar conectividad y estado
kubectl cluster-info
kubectl get nodes -o wide

# 5. Verificar que tienes 8 nodos listos
echo "‚úÖ Verificando que todos los nodos est√©n Ready:"
kubectl get nodes | grep Ready | wc -l
# Debe mostrar: 8
```

### 2.2 Configurar Contexto y Permisos

```bash
# 1. Configurar contexto actual
kubectl config current-context

# 2. Crear ClusterRoleBinding para admin
kubectl create clusterrolebinding cluster-admin-binding \
    --clusterrole=cluster-admin \
    --user=$(gcloud config get-value account)

# 3. Verificar permisos
kubectl auth can-i create pods --all-namespaces
kubectl auth can-i create deployments --all-namespaces

# 4. Configurar Docker para Google Container Registry
gcloud auth configure-docker

echo "‚úÖ Cluster GKE creado y configurado exitosamente"
echo "üìä Nodos disponibles: $(kubectl get nodes --no-headers | wc -l)"
echo "üîß Listo para el siguiente paso: Namespaces"
```

### 2.2 Configurar Permisos de Usuario

```bash
# 1. Obtener tu email de Google Cloud
export USER_EMAIL=$(gcloud config get-value account)

# 2. Crear ClusterRoleBinding para admin
kubectl create clusterrolebinding cluster-admin-binding \
    --clusterrole=cluster-admin \
    --user=$USER_EMAIL

# 3. Verificar permisos
kubectl auth can-i create pods --all-namespaces
```

---

## üì¶ PASO 3: CONFIGURACI√ìN DE NAMESPACES Y BASE

### 3.1 Crear Estructura de Namespaces

```bash
# 1. Crear todos los namespaces necesarios
kubectl apply -f k8s/namespaces/

# 2. Si no existen los archivos, crearlos:
mkdir -p k8s/namespaces
cat > k8s/namespaces/all-namespaces.yaml <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    environment: development
    pod-security.kubernetes.io/enforce: baseline
    pod-security.kubernetes.io/audit: baseline
    pod-security.kubernetes.io/warn: baseline
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    environment: monitoring
---
apiVersion: v1
kind: Namespace
metadata:
  name: logging
  labels:
    environment: logging
---
apiVersion: v1
kind: Namespace
metadata:
  name: keda
  labels:
    environment: autoscaling
---
apiVersion: v1
kind: Namespace
metadata:
  name: sealed-secrets
  labels:
    environment: security
---
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx
  labels:
    environment: networking
EOF

# 3. Aplicar la configuraci√≥n
kubectl apply -f k8s/namespaces/all-namespaces.yaml

# 4. Verificar creaci√≥n y etiquetas
kubectl get namespaces --show-labels

echo "‚úÖ Namespaces creados exitosamente:"
kubectl get namespaces | grep -E "(dev|monitoring|logging|keda|sealed-secrets|ingress-nginx)"
```

### 3.2 Instalar Ingress Controller

```bash
# 1. Agregar repositorio Helm de Nginx
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

# 2. Instalar Nginx Ingress Controller
helm install ingress-nginx ingress-nginx/ingress-nginx \
    --namespace ingress-nginx \
    --create-namespace \
    --set controller.service.type=LoadBalancer

# 3. Esperar a que est√© listo
kubectl wait --namespace ingress-nginx \
    --for=condition=ready pod \
    --selector=app.kubernetes.io/component=controller \
    --timeout=300s

# 4. Obtener IP externa
kubectl get svc -n ingress-nginx ingress-nginx-controller
```

### 3.3 Instalar Helm Repositories

```bash
# Agregar todos los repos necesarios
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add kedacore https://kedacore.github.io/charts
helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets
helm repo update
```

---

## üîê PASO 4: CONFIGURACI√ìN DE SEGURIDAD

### 4.1 Aplicar Pod Security Standards

```bash
# Aplicar pol√≠ticas de seguridad
kubectl apply -f k8s/security/pod-security-standards.yaml

# Verificar aplicaci√≥n
kubectl get namespaces --show-labels
```

### 4.2 Instalar Sealed Secrets

```bash
# 1. Instalar Sealed Secrets Controller
helm install sealed-secrets sealed-secrets/sealed-secrets \
    --namespace sealed-secrets \
    --create-namespace

# 2. Verificar instalaci√≥n
kubectl get pods -n sealed-secrets
kubectl logs -n sealed-secrets deployment/sealed-secrets-controller
```

### 4.3 Aplicar Network Policies

```bash
# Aplicar pol√≠ticas de red
kubectl apply -f k8s/security/network-policies.yaml

# Verificar pol√≠ticas
kubectl get networkpolicy -n dev
```

### 4.4 Configurar RBAC

```bash
# Aplicar roles y bindings
kubectl apply -f k8s/rbac/

# Verificar RBAC
kubectl get clusterroles | grep ecommerce
kubectl get rolebindings -n dev
```

---

## üìä PASO 5: DESPLIEGUE DEL STACK DE MONITOREO

### 5.1 Instalar Prometheus

```bash
# 1. Instalar Prometheus con configuraci√≥n personalizada
helm install prometheus prometheus-community/kube-prometheus-stack \
    --namespace monitoring \
    --create-namespace \
    --values k8s/monitoring/prometheus-values.yaml

# 2. Verificar instalaci√≥n
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

### 5.2 Configurar Grafana

```bash
# 1. Obtener password de admin de Grafana
kubectl get secret -n monitoring prometheus-grafana \
    -o jsonpath="{.data.admin-password}" | base64 --decode
echo

# 2. Hacer port-forward para acceder (opcional)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80 &

# 3. Aplicar dashboards personalizados
kubectl apply -f k8s/monitoring/grafana-dashboards/
```

### 5.3 Instalar Sistema de Logging

```bash
# 1. Aplicar configuraci√≥n de Loki
kubectl apply -f k8s/monitoring/loki-deployment.yaml

# 2. Aplicar DaemonSet de Promtail
kubectl apply -f k8s/monitoring/promtail-simple.yaml

# 3. Verificar logging stack
kubectl get pods -n logging
kubectl logs -n logging deployment/loki
```

---

## ‚ö° PASO 6: CONFIGURACI√ìN DE AUTOSCALING

### 6.1 Instalar KEDA

```bash
# 1. Instalar KEDA v√≠a Helm
helm install keda kedacore/keda \
    --namespace keda \
    --create-namespace

# 2. Verificar instalaci√≥n
kubectl get pods -n keda
kubectl get crd | grep keda
```

### 6.2 Configurar Metrics Server (si no est√° instalado)

```bash
# Verificar si metrics server existe
kubectl get deployment metrics-server -n kube-system

# Si no existe, instalarlo
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

---

## üóÑÔ∏è PASO 7: CONFIGURACI√ìN DE ALMACENAMIENTO

### 7.1 Crear StorageClass

```bash
# Aplicar StorageClass personalizada
kubectl apply -f k8s/storage/gke-storage-class.yaml

# Verificar creaci√≥n
kubectl get storageclass
```

### 7.2 Crear Persistent Volumes

```bash
# Aplicar PVs y PVCs
kubectl apply -f k8s/storage/

# Verificar vol√∫menes
kubectl get pv,pvc -A
```

---

## üöÄ PASO 8: DESPLIEGUE COMPLETO DE MICROSERVICIOS

### 8.1 Crear Base de Datos PostgreSQL

```bash
# 1. Crear ConfigMap para PostgreSQL
cat > k8s/deployments/postgres.yaml <<EOF
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: dev
data:
  POSTGRES_DB: ecommerce_db
  POSTGRES_USER: ecommerce
---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: dev
type: Opaque
data:
  POSTGRES_PASSWORD: ZWNvbW1lcmNlMTIz  # ecommerce123 en base64
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: dev
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: dev
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13-alpine
        ports:
        - containerPort: 5432
          name: postgres
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: dev
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
EOF

# 2. Desplegar PostgreSQL
kubectl apply -f k8s/deployments/postgres.yaml

# 3. Esperar a que PostgreSQL est√© listo
echo "‚è±Ô∏è Esperando que PostgreSQL est√© listo..."
kubectl wait --for=condition=ready pod -l app=postgres -n dev --timeout=300s

# 4. Verificar PostgreSQL
kubectl get pods -n dev -l app=postgres
kubectl logs -n dev -l app=postgres --tail=10

echo "‚úÖ PostgreSQL desplegado exitosamente"
```

### 8.2 Aplicar ConfigMaps y Secrets para Microservicios

```bash
# 1. Crear directorio si no existe
mkdir -p k8s/configmaps k8s/secrets

# 2. Aplicar todas las configuraciones existentes
if [ -d "k8s/configmaps" ] && [ "$(ls -A k8s/configmaps)" ]; then
    kubectl apply -f k8s/configmaps/
fi

# 3. Crear secrets b√°sicos si no existen
kubectl create secret generic database-credentials \
    --from-literal=username=ecommerce \
    --from-literal=password=ecommerce123 \
    --namespace=dev \
    --dry-run=client -o yaml | kubectl apply -f -

# 4. Verificar configuraciones
kubectl get configmaps -n dev
kubectl get secrets -n dev

echo "‚úÖ ConfigMaps y Secrets configurados"
```

### 8.3 Construir y Desplegar Todos los Microservicios

```bash
# 1. Script autom√°tico para construir todas las im√°genes Docker
echo "üî® Construyendo todas las im√°genes Docker..."

# Lista de servicios en orden de dependencias
SERVICES=(
    "service-discovery:8761"
    "cloud-config:9296" 
    "user-service:8700"
    "product-service:8500"
    "order-service:8300"
    "payment-service:8400"
    "shipping-service:8600"
    "favourite-service:8800"
    "proxy-client:8900"
    "api-gateway:8080"
    "frontend:80"
)

# 2. Construir im√°genes usando Cloud Build
for service_info in "${SERVICES[@]}"; do
    service_name="${service_info%:*}"
    port="${service_info#*:}"
    
    echo "üî® Construyendo $service_name..."
    
    # Crear cloudbuild.yaml din√°micamente
    cat > cloudbuild-$service_name.yaml <<EOF
steps:
  - name: 'gcr.io/cloud-builders/mvn'
    args: ['clean', 'package', '-DskipTests', '-f', '$service_name/pom.xml']
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/$service_name:latest', 
           '-t', 'gcr.io/$PROJECT_ID/$service_name:1.0.0', 
           '-f', '$service_name/Dockerfile', '.']

images:
  - 'gcr.io/$PROJECT_ID/$service_name:latest'
  - 'gcr.io/$PROJECT_ID/$service_name:1.0.0'

timeout: 1200s
options:
  machineType: 'E2_HIGHCPU_8'
  diskSizeGb: 100
EOF
    
    # Construir imagen
    gcloud builds submit --config=cloudbuild-$service_name.yaml . &
done

# 3. Esperar a que todas las construcciones terminen
wait
echo "‚úÖ Todas las im√°genes construidas exitosamente"

# 4. Verificar im√°genes en el registry
gcloud container images list --repository=gcr.io/$PROJECT_ID
```

### 8.4 Desplegar Servicios de Infraestructura

```bash
# 1. Service Discovery (Eureka Server)
echo "üöÄ Desplegando Service Discovery (Eureka)..."

cat > k8s/deployments/service-discovery.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: service-discovery
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: service-discovery
  template:
    metadata:
      labels:
        app: service-discovery
    spec:
      containers:
      - name: service-discovery
        image: gcr.io/$PROJECT_ID/service-discovery:latest
        ports:
        - containerPort: 8761
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "dev"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8761
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: 8761
          initialDelaySeconds: 60
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: service-discovery
  namespace: dev
spec:
  selector:
    app: service-discovery
  ports:
  - port: 8761
    targetPort: 8761
  type: ClusterIP
EOF

kubectl apply -f k8s/deployments/service-discovery.yaml

# 2. Cloud Config Server
echo "üöÄ Desplegando Cloud Config Server..."

cat > k8s/deployments/cloud-config.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud-config
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cloud-config
  template:
    metadata:
      labels:
        app: cloud-config
    spec:
      containers:
      - name: cloud-config
        image: gcr.io/$PROJECT_ID/cloud-config:latest
        ports:
        - containerPort: 9296
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "dev"
        - name: EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE
          value: "http://service-discovery:8761/eureka/"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 9296
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: 9296
          initialDelaySeconds: 60
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: cloud-config
  namespace: dev
spec:
  selector:
    app: cloud-config
  ports:
  - port: 9296
    targetPort: 9296
  type: ClusterIP
EOF

kubectl apply -f k8s/deployments/cloud-config.yaml

# 3. Esperar a que los servicios base est√©n listos
echo "‚è±Ô∏è Esperando que los servicios de infraestructura est√©n listos..."
kubectl wait --for=condition=ready pod -l app=service-discovery -n dev --timeout=300s
kubectl wait --for=condition=ready pod -l app=cloud-config -n dev --timeout=300s

echo "‚úÖ Servicios de infraestructura desplegados exitosamente"
```

### 8.5 Desplegar Microservicios de Negocio

```bash
# Script para desplegar todos los microservicios de negocio
echo "üöÄ Desplegando microservicios de negocio..."

# Funci√≥n para crear deployment gen√©rico
create_microservice_deployment() {
    local service=$1
    local port=$2
    local db_name=$3
    
    cat > k8s/deployments/$service.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: $service
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: $service
  template:
    metadata:
      labels:
        app: $service
    spec:
      initContainers:
      - name: wait-for-eureka
        image: curlimages/curl:latest
        command: ['sh', '-c']
        args:
          - |
            echo "Esperando Service Discovery..."
            until curl -f http://service-discovery:8761/actuator/health; do
              echo "Service Discovery no disponible, esperando..."
              sleep 10
            done
            echo "Service Discovery disponible!"
      - name: wait-for-postgres
        image: postgres:13-alpine
        command: ['sh', '-c']
        args:
          - |
            echo "Esperando PostgreSQL..."
            until pg_isready -h postgres -p 5432 -U ecommerce; do
              echo "PostgreSQL no disponible, esperando..."
              sleep 5
            done
            echo "PostgreSQL disponible!"
        env:
        - name: PGPASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
      containers:
      - name: $service
        image: gcr.io/$PROJECT_ID/$service:latest
        ports:
        - containerPort: $port
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "dev"
        - name: EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE
          value: "http://service-discovery:8761/eureka/"
        - name: SPRING_CONFIG_IMPORT
          value: "optional:configserver:http://cloud-config:9296/"
        - name: SPRING_DATASOURCE_URL
          value: "jdbc:postgresql://postgres:5432/$db_name"
        - name: SPRING_DATASOURCE_USERNAME
          value: "ecommerce"
        - name: SPRING_DATASOURCE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        startupProbe:
          httpGet:
            path: /actuator/health
            port: $port
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 30
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: $port
          initialDelaySeconds: 120
          periodSeconds: 30
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: $port
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: $service
  namespace: dev
spec:
  selector:
    app: $service
  ports:
  - port: $port
    targetPort: $port
  type: ClusterIP
EOF
}

# Crear y desplegar cada microservicio
BUSINESS_SERVICES=(
    "user-service:8700:user_db"
    "product-service:8500:product_db"
    "order-service:8300:order_db"
    "payment-service:8400:payment_db"
    "shipping-service:8600:shipping_db"
    "favourite-service:8800:favourite_db"
)

for service_info in "${BUSINESS_SERVICES[@]}"; do
    IFS=':' read -r service port db_name <<< "$service_info"
    
    echo "üöÄ Desplegando $service en puerto $port con base de datos $db_name..."
    
    # Crear base de datos si no existe
    kubectl exec -n dev postgres-0 -- psql -U ecommerce -d postgres -c "CREATE DATABASE $db_name;" 2>/dev/null || echo "Base de datos $db_name ya existe"
    
    # Crear deployment
    create_microservice_deployment $service $port $db_name
    kubectl apply -f k8s/deployments/$service.yaml
    
    # Esperar a que est√© listo antes del siguiente
    echo "‚è±Ô∏è Esperando que $service est√© listo..."
    kubectl wait --for=condition=ready pod -l app=$service -n dev --timeout=600s
    
    echo "‚úÖ $service desplegado exitosamente"
done

echo "üéâ Todos los microservicios de negocio desplegados exitosamente"
```

### 8.6 Desplegar Gateway y Frontend

```bash
# 1. Desplegar Proxy Client
echo "üöÄ Desplegando Proxy Client..."

cat > k8s/deployments/proxy-client.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: proxy-client
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: proxy-client
  template:
    metadata:
      labels:
        app: proxy-client
    spec:
      containers:
      - name: proxy-client
        image: gcr.io/$PROJECT_ID/proxy-client:latest
        ports:
        - containerPort: 8900
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "dev"
        - name: EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE
          value: "http://service-discovery:8761/eureka/"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
        startupProbe:
          httpGet:
            path: /actuator/health
            port: 8900
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8900
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: 8900
          initialDelaySeconds: 60
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: proxy-client
  namespace: dev
spec:
  selector:
    app: proxy-client
  ports:
  - port: 8900
    targetPort: 8900
  type: ClusterIP
EOF

kubectl apply -f k8s/deployments/proxy-client.yaml

# 2. Desplegar API Gateway con LoadBalancer
echo "üöÄ Desplegando API Gateway..."

cat > k8s/deployments/api-gateway.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  namespace: dev
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
      - name: api-gateway
        image: gcr.io/$PROJECT_ID/api-gateway:latest
        ports:
        - containerPort: 8080
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "dev"
        - name: EUREKA_CLIENT_SERVICE_URL_DEFAULTZONE
          value: "http://service-discovery:8761/eureka/"
        - name: SPRING_CONFIG_IMPORT
          value: "optional:configserver:http://cloud-config:9296/"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        startupProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          failureThreshold: 20
        livenessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 120
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
  namespace: dev
spec:
  selector:
    app: api-gateway
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
EOF

kubectl apply -f k8s/deployments/api-gateway.yaml

# 3. Desplegar Frontend
echo "üöÄ Desplegando Frontend..."

cat > k8s/deployments/frontend.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: dev
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - name: frontend
        image: gcr.io/$PROJECT_ID/frontend:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "128Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "100m"
---
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: dev
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 80
  type: LoadBalancer
EOF

kubectl apply -f k8s/deployments/frontend.yaml

# 4. Esperar a que todos est√©n listos
echo "‚è±Ô∏è Esperando que Gateway y Frontend est√©n listos..."
kubectl wait --for=condition=ready pod -l app=proxy-client -n dev --timeout=300s
kubectl wait --for=condition=ready pod -l app=api-gateway -n dev --timeout=300s
kubectl wait --for=condition=ready pod -l app=frontend -n dev --timeout=300s

echo "‚úÖ Gateway y Frontend desplegados exitosamente"
```

---

## ‚öôÔ∏è PASO 9: CONFIGURACI√ìN COMPLETA DE AUTOSCALING

### 9.1 Instalar y Configurar KEDA

```bash
# 1. Instalar KEDA para autoscaling event-driven
helm repo add kedacore https://kedacore.github.io/charts
helm repo update

helm install keda kedacore/keda \
    --namespace keda \
    --create-namespace \
    --set prometheus.metricServer.enabled=true \
    --set prometheus.operator.enabled=true

# 2. Verificar instalaci√≥n de KEDA
kubectl get pods -n keda
kubectl get crd | grep keda

echo "‚úÖ KEDA instalado exitosamente"
```

### 9.2 Configurar Horizontal Pod Autoscalers (HPA)

```bash
# 1. Verificar que Metrics Server est√© funcionando
kubectl get deployment metrics-server -n kube-system

# 2. Crear HPAs para los microservicios principales
cat > k8s/autoscaling/microservices-hpa.yaml <<EOF
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-gateway-hpa
  namespace: dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-gateway
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 75
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: product-service-hpa
  namespace: dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: product-service
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: order-service-hpa
  namespace: dev
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: order-service
  minReplicas: 1
  maxReplicas: 5
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
EOF

# 3. Aplicar HPAs
mkdir -p k8s/autoscaling
kubectl apply -f k8s/autoscaling/microservices-hpa.yaml

# 4. Verificar HPAs
kubectl get hpa -n dev
kubectl describe hpa api-gateway-hpa -n dev

echo "‚úÖ HPAs configurados exitosamente"
```

### 9.3 Configurar KEDA ScaledObjects (Opcional - Avanzado)

```bash
# 1. Crear ScaledObjects para autoscaling basado en m√©tricas externas
cat > k8s/autoscaling/keda-scaledobjects.yaml <<EOF
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: api-gateway-scaler
  namespace: dev
spec:
  scaleTargetRef:
    name: api-gateway
  minReplicaCount: 2
  maxReplicaCount: 10
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      threshold: "100"
      query: sum(rate(http_server_requests_seconds_total{job="api-gateway"}[2m]))
---
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: user-service-scaler
  namespace: dev
spec:
  scaleTargetRef:
    name: user-service
  minReplicaCount: 1
  maxReplicaCount: 5
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090
      metricName: jvm_memory_usage
      threshold: "0.8"
      query: jvm_memory_used_bytes{job="user-service"} / jvm_memory_max_bytes{job="user-service"}
EOF

# 2. Aplicar ScaledObjects (solo si Prometheus est√° configurado)
kubectl apply -f k8s/autoscaling/keda-scaledobjects.yaml

# 3. Verificar ScaledObjects
kubectl get scaledobjects -n dev
kubectl describe scaledobject api-gateway-scaler -n dev

echo "‚úÖ KEDA ScaledObjects configurados"
```

### 9.4 Probar Autoscaling

```bash
# 1. Generar carga para probar autoscaling
API_GATEWAY_IP=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

if [ ! -z "$API_GATEWAY_IP" ]; then
    echo "üß™ Generando carga para probar autoscaling..."
    
    # Instalar apache bench si no existe
    if ! command -v ab &> /dev/null; then
        echo "Instalando apache bench..."
        # En macOS: brew install httpd
        # En Ubuntu: sudo apt-get install apache2-utils
    fi
    
    # Generar carga
    ab -n 10000 -c 50 http://$API_GATEWAY_IP/actuator/health &
    
    # Monitorear HPA en tiempo real
    echo "üìä Monitoreando HPAs (Ctrl+C para parar):"
    watch -n 5 kubectl get hpa -n dev
else
    echo "‚ö†Ô∏è IP externa no disponible a√∫n, int√©ntalo m√°s tarde"
fi
```

---

## üåê PASO 10: CONFIGURACI√ìN DE INGRESS

### 10.1 Aplicar Ingress Rules

```bash
# Aplicar reglas de Ingress
kubectl apply -f k8s/ingress/

# Verificar Ingress
kubectl get ingress -n dev
kubectl describe ingress ecommerce-ingress -n dev
```

### 10.2 Configurar DNS (Opcional)

```bash
# Obtener IP externa del Load Balancer
export INGRESS_IP=$(kubectl get svc -n ingress-nginx ingress-nginx-controller \
    -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

echo "Configura tu DNS para apuntar a: $INGRESS_IP"
echo "Ejemplo: api.ecommerce.local -> $INGRESS_IP"
```

---

## ‚úÖ PASO 11: VERIFICACI√ìN COMPLETA Y TESTING

### 11.1 Verificar Estado General del Sistema

```bash
# Script completo de verificaci√≥n del despliegue
cat > verify-deployment.sh <<'EOF'
#!/bin/bash
echo "üîç VERIFICACI√ìN COMPLETA DEL DESPLIEGUE E-COMMERCE MICROSERVICES"
echo "=================================================================="
echo ""

# 1. Verificar cluster
echo "1Ô∏è‚É£ Estado del cluster:"
kubectl cluster-info --context=$(kubectl config current-context) | head -3
kubectl get nodes -o wide
echo ""

# 2. Verificar namespaces
echo "2Ô∏è‚É£ Namespaces del proyecto:"
kubectl get namespaces | grep -E "(dev|monitoring|logging|keda|sealed-secrets|ingress-nginx)"
echo ""

# 3. Verificar todos los pods
echo "3Ô∏è‚É£ Estado de todos los microservicios:"
kubectl get pods -n dev -o wide
echo ""

# 4. Contar pods por estado
RUNNING=$(kubectl get pods -n dev --no-headers | grep Running | wc -l)
PENDING=$(kubectl get pods -n dev --no-headers | grep Pending | wc -l)
ERROR=$(kubectl get pods -n dev --no-headers | grep -E "Error|CrashLoop|ImagePull" | wc -l)

echo "üìä Resumen de pods:"
echo "   ‚úÖ Running: $RUNNING"
echo "   ‚è≥ Pending: $PENDING" 
echo "   ‚ùå Con errores: $ERROR"
echo ""

# 5. Verificar servicios y IPs externas
echo "4Ô∏è‚É£ Servicios y acceso externo:"
kubectl get svc -n dev
echo ""

# 6. Obtener IPs externas importantes
API_GATEWAY_IP=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
FRONTEND_IP=$(kubectl get svc frontend -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)

if [ ! -z "$API_GATEWAY_IP" ]; then
    echo "üåê API Gateway IP: $API_GATEWAY_IP"
    echo "   Prueba: curl http://$API_GATEWAY_IP/actuator/health"
fi

if [ ! -z "$FRONTEND_IP" ]; then
    echo "üåê Frontend IP: $FRONTEND_IP"
    echo "   Acceso: http://$FRONTEND_IP"
fi
echo ""

# 7. Verificar bases de datos
echo "5Ô∏è‚É£ Verificando bases de datos:"
kubectl exec -n dev postgres-0 -- psql -U ecommerce -d postgres -c "\l" | grep -E "(user_db|product_db|order_db|payment_db|shipping_db|favourite_db)"
echo ""

# 8. Verificar registro en Eureka
echo "6Ô∏è‚É£ Verificando Service Discovery:"
echo "   Port-forward: kubectl port-forward -n dev svc/service-discovery 8761:8761"
echo "   URL: http://localhost:8761"
echo ""

# 9. Verificar recursos del cluster
echo "7Ô∏è‚É£ Uso de recursos:"
kubectl top nodes 2>/dev/null || echo "Metrics server no disponible"
echo ""

# 10. Mostrar eventos recientes
echo "8Ô∏è‚É£ Eventos recientes (√∫ltimos 10):"
kubectl get events -n dev --sort-by='.lastTimestamp' | tail -10
echo ""

echo "üéâ VERIFICACI√ìN COMPLETADA"
echo "=========================="
EOF

chmod +x verify-deployment.sh
./verify-deployment.sh
```

### 11.2 Testing de Conectividad y Health Checks

```bash
# 1. Test de health endpoints de todos los servicios
echo "üß™ Testing health endpoints..."

SERVICES=(
    "service-discovery:8761"
    "cloud-config:9296"
    "user-service:8700"
    "product-service:8500"
    "order-service:8300"
    "payment-service:8400"
    "shipping-service:8600"
    "favourite-service:8800"
    "proxy-client:8900"
    "api-gateway:8080"
)

for service_info in "${SERVICES[@]}"; do
    service_name="${service_info%:*}"
    port="${service_info#*:}"
    
    echo "Testing $service_name..."
    
    # Test health endpoint
    kubectl run test-$service_name --image=curlimages/curl --rm -it --restart=Never --quiet -- \
        curl -s -f http://$service_name.$NAMESPACE_DEV.svc.cluster.local:$port/actuator/health > /dev/null
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ $service_name health check OK"
    else
        echo "‚ùå $service_name health check FAILED"
    fi
done

echo ""
echo "üîç Para debugging detallado:"
echo "kubectl logs -n dev -l app=SERVICE_NAME --tail=50"
echo "kubectl describe pod -n dev POD_NAME"
```

### 11.3 Configurar Monitoreo con Zipkin

```bash
# 1. Desplegar Zipkin para tracing distribuido
echo "üìä Desplegando Zipkin para distributed tracing..."

cat > k8s/deployments/zipkin.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
    metadata:
      labels:
        app: zipkin
    spec:
      containers:
      - name: zipkin
        image: openzipkin/zipkin:latest
        ports:
        - containerPort: 9411
        env:
        - name: STORAGE_TYPE
          value: mem
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "250m"
---
apiVersion: v1
kind: Service
metadata:
  name: zipkin
  namespace: dev
spec:
  selector:
    app: zipkin
  ports:
  - port: 9411
    targetPort: 9411
  type: LoadBalancer
EOF

kubectl apply -f k8s/deployments/zipkin.yaml

# 2. Esperar a que Zipkin est√© listo
kubectl wait --for=condition=ready pod -l app=zipkin -n dev --timeout=180s

# 3. Obtener IP de Zipkin
ZIPKIN_IP=$(kubectl get svc zipkin -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
echo "üìä Zipkin UI disponible en: http://$ZIPKIN_IP:9411"

echo "‚úÖ Zipkin desplegado exitosamente"
```

### 11.2 Health Checks

```bash
# Verificar health de servicios cr√≠ticos
kubectl get pods -n dev | grep -v Running

# Test de conectividad b√°sica
kubectl run test-pod --image=curlimages/curl --rm -it --restart=Never -- \
    curl -s http://api-gateway.dev.svc.cluster.local:8080/actuator/health

# Verificar m√©tricas de Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090 &
# Acceder a http://localhost:9090 y verificar targets
```

### 11.3 Aplicar Pruebas de Carga (Opcional)

```bash
# Aplicar configuraci√≥n de JMeter
kubectl apply -f k8s/testing/jmeter-load-test.yaml

# Ejecutar test b√°sico
kubectl run jmeter-test --image=justb4/jmeter --rm -it --restart=Never -- \
    jmeter -n -t /test-plans/basic-load-test.jmx
```

---

## üîß PASO 12: CONFIGURACI√ìN DE CI/CD

### 12.1 Configurar GitHub Actions

```bash
# 1. Crear service account para GitHub Actions
kubectl create serviceaccount github-actions -n dev
kubectl create clusterrolebinding github-actions-binding \
    --clusterrole=cluster-admin \
    --serviceaccount=dev:github-actions

# 2. Obtener token del service account
kubectl get secret $(kubectl get serviceaccount github-actions -n dev \
    -o jsonpath='{.secrets[0].name}') -n dev -o jsonpath='{.data.token}' | base64 --decode
```

### 12.2 Configurar Secrets de GitHub

**En tu repositorio de GitHub, agregar estos secrets:**

```
SECRETS A CONFIGURAR EN GITHUB:
- GCP_PROJECT_ID: tu-project-id
- GKE_CLUSTER: ecommerce-cluster
- GKE_ZONE: us-central1-a
- GCP_SA_KEY: (JSON key del service account)
- KUBE_CONFIG_DATA: (base64 del kubeconfig)
- SLACK_WEBHOOK: (webhook para notificaciones)
```

### 12.3 Activar Workflows

```bash
# Los workflows se activar√°n autom√°ticamente en:
# - Push a main branch
# - Pull requests
# - Dispatch manual

# Verificar workflows en: https://github.com/tu-usuario/tu-repo/actions
```

---

## üìã PASO 13: VALIDACI√ìN FINAL

### 13.1 Checklist de Validaci√≥n

```bash
#!/bin/bash
echo "üîç CHECKLIST DE VALIDACI√ìN FINAL"
echo "================================="

# 1. Cluster y nodos
echo "‚úÖ Verificando cluster..."
kubectl cluster-info > /dev/null && echo "‚úÖ Cluster OK" || echo "‚ùå Cluster ERROR"

# 2. Namespaces
echo "‚úÖ Verificando namespaces..."
NAMESPACES=$(kubectl get ns --no-headers | wc -l)
[[ $NAMESPACES -ge 6 ]] && echo "‚úÖ Namespaces OK ($NAMESPACES)" || echo "‚ùå Namespaces ERROR"

# 3. Microservicios
echo "‚úÖ Verificando microservicios..."
RUNNING_PODS=$(kubectl get pods -n dev --no-headers | grep Running | wc -l)
[[ $RUNNING_PODS -ge 10 ]] && echo "‚úÖ Microservicios OK ($RUNNING_PODS running)" || echo "‚ùå Microservicios ERROR"

# 4. Monitoreo
echo "‚úÖ Verificando monitoreo..."
kubectl get pods -n monitoring --no-headers | grep -q Running && echo "‚úÖ Monitoring OK" || echo "‚ùå Monitoring ERROR"

# 5. Logging
echo "‚úÖ Verificando logging..."
kubectl get pods -n logging --no-headers | grep -q Running && echo "‚úÖ Logging OK" || echo "‚ùå Logging ERROR"

# 6. Autoscaling
echo "‚úÖ Verificando autoscaling..."
kubectl get pods -n keda --no-headers | grep -q Running && echo "‚úÖ KEDA OK" || echo "‚ùå KEDA ERROR"

# 7. Ingress
echo "‚úÖ Verificando ingress..."
kubectl get svc -n ingress-nginx --no-headers | grep -q LoadBalancer && echo "‚úÖ Ingress OK" || echo "‚ùå Ingress ERROR"

echo ""
echo "üéâ VALIDACI√ìN COMPLETADA"
echo "========================"
```

### 13.2 URLs de Acceso y Informaci√≥n de Conectividad

```bash
# Script para obtener todas las URLs importantes
cat > get-access-urls.sh <<'EOF'
#!/bin/bash
echo "üåê URLS DE ACCESO DEL E-COMMERCE MICROSERVICES"
echo "==============================================="
echo ""

# Obtener IPs externas
API_GATEWAY_IP=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
FRONTEND_IP=$(kubectl get svc frontend -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
ZIPKIN_IP=$(kubectl get svc zipkin -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
GRAFANA_IP=$(kubectl get svc prometheus-grafana -n monitoring -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)

echo "üöÄ APLICACI√ìN PRINCIPAL:"
if [ ! -z "$API_GATEWAY_IP" ]; then
    echo "   API Gateway: http://$API_GATEWAY_IP"
    echo "   Health Check: http://$API_GATEWAY_IP/actuator/health"
    echo "   Swagger UI: http://$API_GATEWAY_IP/swagger-ui.html"
else
    echo "   ‚è≥ API Gateway LoadBalancer a√∫n no tiene IP externa"
fi

if [ ! -z "$FRONTEND_IP" ]; then
    echo "   Frontend Web: http://$FRONTEND_IP"
else
    echo "   ‚è≥ Frontend LoadBalancer a√∫n no tiene IP externa"
fi

echo ""
echo "üìä MONITOREO Y OBSERVABILIDAD:"

if [ ! -z "$ZIPKIN_IP" ]; then
    echo "   Zipkin Tracing: http://$ZIPKIN_IP:9411"
else
    echo "   ‚è≥ Zipkin LoadBalancer a√∫n no tiene IP externa"
    echo "   Port-forward: kubectl port-forward -n dev svc/zipkin 9411:9411"
fi

if [ ! -z "$GRAFANA_IP" ]; then
    echo "   Grafana Dashboards: http://$GRAFANA_IP"
    echo "   Credenciales: admin / (obtener con siguiente comando)"
else
    echo "   Grafana: kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80"
fi

echo "   Prometheus: kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090"
echo "   Eureka Discovery: kubectl port-forward -n dev svc/service-discovery 8761:8761"

echo ""
echo "üîë CREDENCIALES:"
echo "   Grafana Password:"
kubectl get secret -n monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" 2>/dev/null | base64 --decode 2>/dev/null || echo "     (Grafana no instalado)"
echo ""

echo ""
echo "üß™ TESTING ENDPOINTS:"
if [ ! -z "$API_GATEWAY_IP" ]; then
    echo "   curl http://$API_GATEWAY_IP/actuator/health"
    echo "   curl http://$API_GATEWAY_IP/user-service/actuator/health"
    echo "   curl http://$API_GATEWAY_IP/product-service/actuator/health"
fi

echo ""
echo "üì± COMANDOS √öTILES:"
echo "   Ver todos los servicios: kubectl get svc -n dev"
echo "   Ver logs: kubectl logs -n dev -l app=SERVICE_NAME"
echo "   Escalar servicio: kubectl scale deployment SERVICE_NAME -n dev --replicas=3"
echo "   Ver HPA: kubectl get hpa -n dev"

echo ""
echo "üí° NOTA: Si las IPs externas muestran <pending>, espera unos minutos"
echo "   y ejecuta este script nuevamente."
EOF

chmod +x get-access-urls.sh
./get-access-urls.sh

# Guardar URLs en archivo para referencia
./get-access-urls.sh > URLS_ACCESO.txt
echo ""
echo "üìù URLs guardadas en: URLS_ACCESO.txt"
```

### 13.3 Configuraci√≥n DNS Local (Opcional)

```bash
# Para acceso m√°s f√°cil, agregar entradas al archivo hosts local
API_GATEWAY_IP=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
FRONTEND_IP=$(kubectl get svc frontend -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

if [ ! -z "$API_GATEWAY_IP" ] && [ ! -z "$FRONTEND_IP" ]; then
    echo "üåê Agregando entradas DNS locales..."
    
    # Crear script para actualizar hosts
    cat > update-hosts.sh <<EOF
#!/bin/bash
# Backup del archivo hosts
sudo cp /etc/hosts /etc/hosts.backup

# Agregar entradas para el proyecto
echo "" | sudo tee -a /etc/hosts
echo "# E-commerce Microservices Project" | sudo tee -a /etc/hosts
echo "$API_GATEWAY_IP    api.ecommerce.local" | sudo tee -a /etc/hosts
echo "$FRONTEND_IP       frontend.ecommerce.local" | sudo tee -a /etc/hosts

echo "‚úÖ DNS local configurado"
echo "Ahora puedes usar:"
echo "  http://api.ecommerce.local"
echo "  http://frontend.ecommerce.local"
EOF
    
    chmod +x update-hosts.sh
    echo "Ejecuta: sudo ./update-hosts.sh para configurar DNS local"
else
    echo "‚è≥ Esperando IPs externas para configurar DNS local"
fi
```

---

## üÜò TROUBLESHOOTING COMPLETO Y SOLUCIONES

### üîß Script de Diagn√≥stico Autom√°tico

```bash
# Crear script de diagn√≥stico completo
cat > diagnose-issues.sh <<'EOF'
#!/bin/bash
echo "üîç DIAGN√ìSTICO AUTOM√ÅTICO DE PROBLEMAS"
echo "======================================"
echo ""

# 1. Verificar recursos del cluster
echo "1Ô∏è‚É£ RECURSOS DEL CLUSTER:"
kubectl top nodes 2>/dev/null || echo "Metrics server no disponible"
kubectl describe nodes | grep -A 5 "Allocated resources" | head -20
echo ""

# 2. Pods con problemas
echo "2Ô∏è‚É£ PODS CON PROBLEMAS:"
kubectl get pods -n dev | grep -v Running | grep -v Completed
echo ""

# 3. Eventos recientes de error
echo "3Ô∏è‚É£ EVENTOS DE ERROR RECIENTES:"
kubectl get events -n dev --field-selector type=Warning --sort-by='.lastTimestamp' | tail -10
echo ""

# 4. Verificar im√°genes
echo "4Ô∏è‚É£ VERIFICANDO IM√ÅGENES EN REGISTRY:"
gcloud container images list --repository=gcr.io/$PROJECT_ID --limit=5
echo ""

# 5. Servicios sin endpoints
echo "5Ô∏è‚É£ SERVICIOS SIN ENDPOINTS:"
for svc in $(kubectl get svc -n dev --no-headers | awk '{print $1}'); do
    endpoints=$(kubectl get endpoints $svc -n dev --no-headers | awk '{print $2}')
    if [ "$endpoints" == "<none>" ]; then
        echo "‚ùå $svc no tiene endpoints"
    fi
done
echo ""

# 6. LoadBalancers pendientes
echo "6Ô∏è‚É£ LOADBALANCERS PENDIENTES:"
kubectl get svc -n dev --no-headers | grep LoadBalancer | grep '<pending>'
echo ""

echo "‚úÖ DIAGN√ìSTICO COMPLETADO"
EOF

chmod +x diagnose-issues.sh
./diagnose-issues.sh
```

### üö® Problemas Comunes y Soluciones

#### Problema 1: Pods en Pending - Recursos Insuficientes

```bash
# S√≠ntomas: Pods stuck en Pending
# Causa: Insuficientes recursos CPU/Memory

# Soluci√≥n 1: Verificar y liberar recursos
kubectl describe nodes | grep -A 10 "Non-terminated pods"
kubectl delete pods -n dev --field-selector status.phase=Failed
kubectl delete pods -n dev --field-selector status.phase=Succeeded

# Soluci√≥n 2: Reducir requests de recursos
kubectl patch deployment user-service -n dev -p '{"spec":{"template":{"spec":{"containers":[{"name":"user-service","resources":{"requests":{"cpu":"100m","memory":"256Mi"}}}]}}}}'

# Soluci√≥n 3: Escalar el cluster
gcloud container clusters resize $CLUSTER_NAME --num-nodes=10 --zone=$ZONE
```

#### Problema 2: ImagePullBackOff - Problemas de Registry

```bash
# S√≠ntomas: Pods muestran ImagePullBackOff o ErrImagePull
# Causa: Imagen no existe o problemas de autenticaci√≥n

# Soluci√≥n 1: Verificar autenticaci√≥n
gcloud auth configure-docker
gcloud auth print-access-token | docker login -u oauth2accesstoken --password-stdin https://gcr.io

# Soluci√≥n 2: Verificar que la imagen existe
gcloud container images list --repository=gcr.io/$PROJECT_ID
gcloud container images list-tags gcr.io/$PROJECT_ID/user-service

# Soluci√≥n 3: Reconstruir imagen
gcloud builds submit --config=cloudbuild-user-service.yaml .

# Soluci√≥n 4: Usar imagen alternativa temporalmente
kubectl set image deployment/user-service user-service=gcr.io/$PROJECT_ID/user-service:latest -n dev
```

#### Problema 3: CrashLoopBackOff - Aplicaci√≥n no Inicia

```bash
# S√≠ntomas: Pods se reinician constantemente
# Causa: Error en la aplicaci√≥n, dependencias no disponibles

# Diagn√≥stico detallado
kubectl describe pod -n dev $(kubectl get pods -n dev | grep user-service | awk '{print $1}')
kubectl logs -n dev -l app=user-service --previous --tail=100

# Soluci√≥n 1: Verificar dependencias
kubectl get pods -n dev | grep -E "(postgres|service-discovery|cloud-config)"

# Soluci√≥n 2: Aumentar tiempos de probe
kubectl patch deployment user-service -n dev -p '{
  "spec": {
    "template": {
      "spec": {
        "containers": [{
          "name": "user-service",
          "startupProbe": {
            "failureThreshold": 60,
            "periodSeconds": 10
          }
        }]
      }
    }
  }
}'

# Soluci√≥n 3: Verificar configuraci√≥n de variables de entorno
kubectl describe deployment user-service -n dev | grep -A 20 "Environment:"
```

#### Problema 4: Services no Responden - Network Issues

```bash
# S√≠ntomas: Timeouts al conectar a servicios
# Causa: Network policies, DNS, o configuraci√≥n incorrecta

# Diagn√≥stico de conectividad
kubectl run debug-pod --image=curlimages/curl --rm -it --restart=Never -- sh

# Desde el pod debug:
# curl http://user-service.dev.svc.cluster.local:8700/actuator/health
# nslookup user-service.dev.svc.cluster.local

# Verificar endpoints y servicios
kubectl get endpoints -n dev
kubectl describe svc user-service -n dev

# Verificar network policies
kubectl get networkpolicy -n dev
kubectl describe networkpolicy -n dev
```

#### Problema 5: LoadBalancer IP Pendiente

```bash
# S√≠ntomas: External IP shows <pending>
# Causa: Cuotas de GCP, region sin LB, configuraci√≥n incorrecta

# Verificar cuotas
gcloud compute project-info describe --project=$PROJECT_ID

# Verificar que la regi√≥n soporte LoadBalancers
gcloud compute regions describe us-central1

# Soluci√≥n temporal: usar NodePort
kubectl patch svc api-gateway -n dev -p '{"spec":{"type":"NodePort"}}'

# Obtener IP de nodo + puerto
kubectl get nodes -o wide
kubectl get svc api-gateway -n dev
```

#### Problema 6: Base de Datos Connection Issues

```bash
# S√≠ntomas: Microservicios no pueden conectar a PostgreSQL
# Causa: PostgreSQL no ready, credenciales incorrectas

# Verificar PostgreSQL
kubectl get pods -n dev -l app=postgres
kubectl logs -n dev postgres-0 --tail=50

# Test de conectividad a DB
kubectl exec -n dev postgres-0 -- pg_isready -U ecommerce

# Verificar secretos
kubectl get secret postgres-secret -n dev -o yaml

# Recrear secret si es necesario
kubectl delete secret postgres-secret -n dev
kubectl create secret generic postgres-secret \
  --from-literal=POSTGRES_PASSWORD=ecommerce123 \
  --namespace=dev
```

### üîÑ Scripts de Recuperaci√≥n Autom√°tica

```bash
# Script de restart inteligente
cat > smart-restart.sh <<'EOF'
#!/bin/bash
echo "üîÑ RESTART INTELIGENTE DE SERVICIOS"
echo "=================================="

# Restart servicios en orden de dependencias
SERVICES=("postgres" "service-discovery" "cloud-config" "user-service" "product-service" "order-service" "payment-service" "shipping-service" "favourite-service" "proxy-client" "api-gateway" "frontend")

for service in "${SERVICES[@]}"; do
    echo "üîÑ Restarting $service..."
    kubectl rollout restart deployment/$service -n dev 2>/dev/null || echo "‚ö†Ô∏è $service no es deployment"
    
    if [ "$service" != "postgres" ] && [ "$service" != "frontend" ]; then
        echo "‚è≥ Esperando que $service est√© ready..."
        kubectl wait --for=condition=ready pod -l app=$service -n dev --timeout=300s
    fi
    
    echo "‚úÖ $service restarted"
done

echo "üéâ Restart completo terminado"
EOF

chmod +x smart-restart.sh

# Script de limpieza
cat > cleanup-failed-pods.sh <<'EOF'
#!/bin/bash
echo "üßπ LIMPIEZA DE PODS FALLIDOS"
echo "============================"

# Eliminar pods fallidos
kubectl delete pods -n dev --field-selector status.phase=Failed
kubectl delete pods -n dev --field-selector status.phase=Succeeded

# Eliminar pods en ImagePullBackOff por m√°s de 10 minutos
kubectl get pods -n dev --field-selector status.phase=Pending -o json | \
jq -r '.items[] | select(.status.containerStatuses[]?.state.waiting.reason == "ImagePullBackOff") | .metadata.name' | \
xargs -I {} kubectl delete pod {} -n dev

echo "‚úÖ Limpieza completada"
EOF

chmod +x cleanup-failed-pods.sh
```

---

## üßπ CLEANUP (OPCIONAL)

### Eliminar Todo el Despliegue

```bash
# ‚ö†Ô∏è CUIDADO: Esto eliminar√° TODO

# 1. Eliminar aplicaciones
kubectl delete namespace dev
kubectl delete namespace monitoring
kubectl delete namespace logging
kubectl delete namespace keda
kubectl delete namespace sealed-secrets

# 2. Eliminar Ingress Controller
helm uninstall ingress-nginx -n ingress-nginx
kubectl delete namespace ingress-nginx

# 3. Eliminar cluster completo
gcloud container clusters delete $CLUSTER_NAME --zone=$REGION
```

---

## üìû SOPORTE

**Si tienes problemas durante el despliegue:**

1. **Verificar logs:** `kubectl logs -f deployment/nombre-servicio -n dev`
2. **Describir recursos:** `kubectl describe pod nombre-pod -n dev`
3. **Verificar eventos:** `kubectl get events --sort-by='.lastTimestamp' -n dev`
4. **Consultar documentaci√≥n:** Ver `DOCUMENTACION-PROYECTO-FINAL.md`

**Tiempo estimado de despliegue completo:** 45-60 minutos

**Recursos necesarios:** ~$50-100/mes en GCP para ambiente de desarrollo

## ü§ñ SCRIPT DE DESPLIEGUE AUTOM√ÅTICO COMPLETO

### üì¶ Despliegue con Un Solo Comando

```bash
# Crear script maestro de despliegue completo
cat > deploy-ecommerce-complete.sh <<'EOF'
#!/bin/bash
set -e  # Exit on any error

echo "üöÄ DESPLIEGUE AUTOM√ÅTICO COMPLETO E-COMMERCE MICROSERVICES"
echo "==========================================================="
echo ""
echo "Este script desplegar√° completamente la plataforma e-commerce"
echo "Tiempo estimado: 45-60 minutos"
echo ""
read -p "¬øContinuar? (y/N): " confirm
if [[ $confirm != [yY] ]]; then
    echo "Despliegue cancelado"
    exit 0
fi

# Cargar variables
source .env 2>/dev/null || {
    echo "‚ùå Archivo .env no encontrado. Ejecuta primero los pasos 1-3 manualmente."
    exit 1
}

echo "üìä Configuraci√≥n:"
echo "   Proyecto: $PROJECT_ID"
echo "   Cluster: $CLUSTER_NAME"
echo "   Regi√≥n: $ZONE"
echo ""

# Funci√≥n para mostrar progreso
show_progress() {
    echo ""
    echo "üéØ PASO $1: $2"
    echo "$(printf '=%.0s' {1..60})"
}

# Funci√≥n para verificar prerequisitos
check_prerequisites() {
    show_progress "0" "VERIFICANDO PREREQUISITOS"
    
    command -v gcloud >/dev/null 2>&1 || { echo "‚ùå gcloud CLI no encontrado"; exit 1; }
    command -v kubectl >/dev/null 2>&1 || { echo "‚ùå kubectl no encontrado"; exit 1; }
    command -v helm >/dev/null 2>&1 || { echo "‚ùå helm no encontrado"; exit 1; }
    
    # Verificar autenticaci√≥n
    gcloud auth list --filter="status:ACTIVE" --format="value(account)" | grep -q . || {
        echo "‚ùå No hay cuenta activa en gcloud. Ejecuta: gcloud auth login"
        exit 1
    }
    
    # Verificar conexi√≥n al cluster
    kubectl cluster-info >/dev/null 2>&1 || {
        echo "‚ùå No conectado al cluster. Ejecuta: gcloud container clusters get-credentials $CLUSTER_NAME --zone=$ZONE"
        exit 1
    }
    
    echo "‚úÖ Todos los prerequisitos verificados"
}

# 1. Crear namespaces
deploy_namespaces() {
    show_progress "1" "CREANDO NAMESPACES"
    
    mkdir -p k8s/namespaces
    cat > k8s/namespaces/all-namespaces.yaml <<YAML
apiVersion: v1
kind: Namespace
metadata:
  name: dev
---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: Namespace
metadata:
  name: keda
YAML
    
    kubectl apply -f k8s/namespaces/all-namespaces.yaml
    echo "‚úÖ Namespaces creados"
}

# 2. Desplegar PostgreSQL
deploy_database() {
    show_progress "2" "DESPLEGANDO BASE DE DATOS"
    
    kubectl apply -f - <<YAML
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: dev
data:
  POSTGRES_DB: ecommerce_db
  POSTGRES_USER: ecommerce
---
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: dev
type: Opaque
data:
  POSTGRES_PASSWORD: ZWNvbW1lcmNlMTIz
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: dev
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: dev
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:13-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            configMapKeyRef:
              name: postgres-config
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: POSTGRES_PASSWORD
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: dev
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
  type: ClusterIP
YAML
    
    kubectl wait --for=condition=ready pod -l app=postgres -n dev --timeout=300s
    echo "‚úÖ PostgreSQL desplegado y listo"
}

# 3. Construir todas las im√°genes
build_images() {
    show_progress "3" "CONSTRUYENDO IM√ÅGENES DOCKER"
    
    SERVICES=("service-discovery" "cloud-config" "user-service" "product-service" "order-service" "payment-service" "shipping-service" "favourite-service" "proxy-client" "api-gateway" "frontend")
    
    for service in "${SERVICES[@]}"; do
        echo "üî® Construyendo $service..."
        gcloud builds submit --config=cloudbuild-$service.yaml . > build-$service.log 2>&1 &
    done
    
    echo "‚è≥ Esperando que terminen todas las construcciones..."
    wait
    echo "‚úÖ Todas las im√°genes construidas"
}

# 4. Desplegar microservicios
deploy_microservices() {
    show_progress "4" "DESPLEGANDO MICROSERVICIOS"
    
    # Servicios de infraestructura primero
    INFRA_SERVICES=("service-discovery:8761" "cloud-config:9296")
    
    for service_info in "${INFRA_SERVICES[@]}"; do
        service_name="${service_info%:*}"
        port="${service_info#*:}"
        
        echo "üöÄ Desplegando $service_name..."
        
        # Aplicar desde archivos k8s si existen, sino crear b√°sico
        if [ -f "k8s/deployments/$service_name.yaml" ]; then
            kubectl apply -f k8s/deployments/$service_name.yaml
        else
            kubectl create deployment $service_name --image=gcr.io/$PROJECT_ID/$service_name:latest -n dev
            kubectl expose deployment $service_name --port=$port --target-port=$port -n dev
        fi
        
        kubectl wait --for=condition=ready pod -l app=$service_name -n dev --timeout=300s
    done
    
    # Servicios de negocio
    BUSINESS_SERVICES=("user-service:8700" "product-service:8500" "order-service:8300" "payment-service:8400" "shipping-service:8600" "favourite-service:8800")
    
    for service_info in "${BUSINESS_SERVICES[@]}"; do
        service_name="${service_info%:*}"
        port="${service_info#*:}"
        
        echo "üöÄ Desplegando $service_name..."
        
        if [ -f "k8s/deployments/$service_name.yaml" ]; then
            kubectl apply -f k8s/deployments/$service_name.yaml
        else
            kubectl create deployment $service_name --image=gcr.io/$PROJECT_ID/$service_name:latest -n dev
            kubectl expose deployment $service_name --port=$port --target-port=$port -n dev
        fi
        
        kubectl wait --for=condition=ready pod -l app=$service_name -n dev --timeout=600s
    done
    
    # Gateways
    GATEWAY_SERVICES=("proxy-client:8900" "api-gateway:8080")
    
    for service_info in "${GATEWAY_SERVICES[@]}"; do
        service_name="${service_info%:*}"
        port="${service_info#*:}"
        
        echo "üöÄ Desplegando $service_name..."
        
        if [ -f "k8s/deployments/$service_name.yaml" ]; then
            kubectl apply -f k8s/deployments/$service_name.yaml
        else
            kubectl create deployment $service_name --image=gcr.io/$PROJECT_ID/$service_name:latest -n dev
            
            if [ "$service_name" = "api-gateway" ]; then
                kubectl expose deployment $service_name --port=80 --target-port=$port --type=LoadBalancer -n dev
            else
                kubectl expose deployment $service_name --port=$port --target-port=$port -n dev
            fi
        fi
        
        kubectl wait --for=condition=ready pod -l app=$service_name -n dev --timeout=300s
    done
    
    echo "‚úÖ Todos los microservicios desplegados"
}

# 5. Configurar autoscaling
setup_autoscaling() {
    show_progress "5" "CONFIGURANDO AUTOSCALING"
    
    # Instalar KEDA si no existe
    if ! kubectl get namespace keda >/dev/null 2>&1; then
        helm repo add kedacore https://kedacore.github.io/charts >/dev/null 2>&1
        helm repo update >/dev/null 2>&1
        helm install keda kedacore/keda --namespace keda --create-namespace >/dev/null 2>&1
    fi
    
    # Crear HPAs b√°sicos
    kubectl autoscale deployment api-gateway --cpu-percent=60 --min=2 --max=10 -n dev >/dev/null 2>&1
    kubectl autoscale deployment user-service --cpu-percent=70 --min=1 --max=5 -n dev >/dev/null 2>&1
    kubectl autoscale deployment product-service --cpu-percent=70 --min=1 --max=5 -n dev >/dev/null 2>&1
    
    echo "‚úÖ Autoscaling configurado"
}

# 6. Verificaci√≥n final
final_verification() {
    show_progress "6" "VERIFICACI√ìN FINAL"
    
    echo "üìä Estado del despliegue:"
    kubectl get pods -n dev
    echo ""
    
    echo "üåê Servicios:"
    kubectl get svc -n dev
    echo ""
    
    echo "üìà HPAs:"
    kubectl get hpa -n dev 2>/dev/null || echo "HPAs no configurados"
    echo ""
    
    API_GATEWAY_IP=$(kubectl get svc api-gateway -n dev -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null)
    
    if [ ! -z "$API_GATEWAY_IP" ]; then
        echo "‚úÖ API Gateway disponible en: http://$API_GATEWAY_IP"
        echo "üß™ Test: curl http://$API_GATEWAY_IP/actuator/health"
    else
        echo "‚è≥ API Gateway IP a√∫n no asignada"
    fi
}

# Ejecuci√≥n principal
main() {
    check_prerequisites
    deploy_namespaces
    deploy_database
    build_images
    deploy_microservices
    setup_autoscaling
    final_verification
    
    echo ""
    echo "üéâ DESPLIEGUE COMPLETO EXITOSO"
    echo "=============================="
    echo ""
    echo "üìã Pr√≥ximos pasos:"
    echo "1. Ejecuta: ./get-access-urls.sh para obtener todas las URLs"
    echo "2. Configura monitoreo con Prometheus/Grafana si lo deseas"
    echo "3. Ejecuta testing con: ./verify-deployment.sh"
    echo ""
    echo "‚è±Ô∏è Tiempo total: $(( SECONDS / 60 )) minutos"
}

# Ejecutar
main
EOF

chmod +x deploy-ecommerce-complete.sh

echo "üì¶ Script de despliegue autom√°tico creado: deploy-ecommerce-complete.sh"
echo ""
echo "üöÄ PARA DESPLEGAR AUTOM√ÅTICAMENTE TODO EL PROYECTO:"
echo "    ./deploy-ecommerce-complete.sh"
echo ""
echo "‚ö†Ô∏è IMPORTANTE: Aseg√∫rate de haber ejecutado los pasos 1-3 manualmente primero"
echo "   (Configuraci√≥n GCP, creaci√≥n cluster, variables de entorno)"
```

---

## üìã RESUMEN Y CHECKLIST FINAL

### ‚úÖ Checklist de Completitud

**Antes del despliegue:**
- [ ] Cuenta GCP con billing habilitado
- [ ] gcloud CLI instalado y autenticado  
- [ ] kubectl instalado
- [ ] Helm instalado
- [ ] Proyecto GCP creado
- [ ] APIs habilitadas (Container, Registry, Compute, Monitoring)

**Durante el despliegue:**
- [ ] Cluster GKE creado (8 nodos e2-medium)
- [ ] Namespaces creados (dev, monitoring, keda)
- [ ] PostgreSQL desplegado y listo
- [ ] Todas las im√°genes construidas en GCR
- [ ] Service Discovery (Eureka) funcionando
- [ ] Cloud Config Server funcionando
- [ ] Todos los microservicios de negocio desplegados
- [ ] API Gateway con LoadBalancer externo
- [ ] Frontend desplegado
- [ ] HPAs configurados
- [ ] Zipkin para tracing (opcional)

**Despu√©s del despliegue:**
- [ ] Todos los pods en estado Running
- [ ] Servicios registrados en Eureka
- [ ] API Gateway accesible externamente
- [ ] Base de datos con esquemas creados
- [ ] Health checks respondiendo
- [ ] LoadBalancer IPs asignadas

### üéØ Informaci√≥n del Proyecto Final

**üìä M√©tricas de Implementaci√≥n:**
- ‚úÖ **11 Microservicios** independientes y escalables
- ‚úÖ **PostgreSQL** con m√∫ltiples bases de datos
- ‚úÖ **Service Discovery** con Eureka
- ‚úÖ **API Gateway** con enrutamiento inteligente
- ‚úÖ **Autoscaling** con HPA y KEDA
- ‚úÖ **Observabilidad** con Zipkin tracing
- ‚úÖ **Cloud-native** en GKE
- ‚úÖ **CI/CD ready** con GitHub Actions
- ‚úÖ **Seguridad** con Network Policies
- ‚úÖ **Alta disponibilidad** con LoadBalancers

**‚è±Ô∏è Tiempos de Despliegue:**
- Manual (siguiendo gu√≠a paso a paso): 60-90 minutos
- Autom√°tico (script completo): 45-60 minutos
- Solo microservicios (sin infraestructura): 30-45 minutos

**üí∞ Costos Estimados (GCP):**
- Desarrollo/Testing: $50-80/mes
- Producci√≥n: $150-250/mes

**üîó Enlaces Importantes:**
- Repositorio: https://github.com/felipevelasco7/ecommerce-microservice-backend-app
- Documentaci√≥n: `docs/`
- Scripts: `scripts/`
- Manifiestos K8s: `k8s/`

---

*Esta gu√≠a completa te permite recrear desde cero la plataforma e-commerce microservices. Cada paso est√° dise√±ado para ser ejecutado independientemente y verificado antes de continuar. Para soporte o dudas, consulta la documentaci√≥n adicional en la carpeta `docs/` o contacta al equipo de desarrollo.*

**√öltima actualizaci√≥n:** 29 de noviembre de 2025  
**Versi√≥n:** 2.0 Unificada  
**Autor:** Felipe Velasco - Universidad Icesi